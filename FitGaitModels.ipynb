{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a905861",
   "metadata": {},
   "source": [
    "# Run Logistic Regression algorithm on extracted IMU feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b9e3c0",
   "metadata": {},
   "source": [
    "## Do relevant imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3132ef43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from utils.extract_features import get_dataset\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import balanced_accuracy_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from joblib import dump, load\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc6d90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENTS = list(range(1,15))\n",
    "REL_LABELS = [-5,-4,-3,-2,-1,0,1,2,3,4]\n",
    "SENSORS = ['wrists', \n",
    "           'ankles',\n",
    "           'no_chest',\n",
    "           'all']\n",
    "NUM_WORKER = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa686b6",
   "metadata": {},
   "source": [
    "## Load feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c55cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, labels = {}, {}\n",
    "for sensors in SENSORS:\n",
    "    dataset[sensors], labels[sensors] = get_dataset(\n",
    "        PATIENTS, \n",
    "        sensors=sensors,\n",
    "        w_size=128, \n",
    "        w_overlap=64, \n",
    "        data_root='/datasets/GaitDetection'\n",
    "        )\n",
    "    for i, p_labels in enumerate(labels[sensors]):\n",
    "        rel_indices = np.isin(labels[sensors][i], REL_LABELS)\n",
    "        dataset[sensors][i] = dataset[sensors][i][rel_indices]\n",
    "        labels[sensors][i] = labels[sensors][i][rel_indices]\n",
    "    for patient_labels in labels[sensors]:\n",
    "            patient_labels[(patient_labels<=2) & (patient_labels>=0)]= 0.\n",
    "            patient_labels[patient_labels>2]= 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316044ff",
   "metadata": {},
   "source": [
    "## Generate training data and leave one out splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ead893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data,target = {}, {}\n",
    "for sensors in dataset:\n",
    "    data[sensors] = np.concatenate(dataset[sensors])\n",
    "    target[sensors] = np.concatenate(labels[sensors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cd70c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSplitter(object):\n",
    "    def __init__(self, splitter, labels):\n",
    "        self.splitter = splitter\n",
    "        self.labels = np.concatenate(labels)\n",
    "    \n",
    "    def split(self, X=None, y=None, groups=None):\n",
    "        \n",
    "        for train_index, test_index in self.splitter.split():\n",
    "            train_index = train_index[self.labels[train_index]>=0]\n",
    "            yield train_index, test_index\n",
    "            \n",
    "    def get_n_splits(self, X=None, y=None, groups=None): \n",
    "        return self.splitter.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d032d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_splits(labels):\n",
    "    # Generate CV splits\n",
    "    test_folds = np.array([])\n",
    "    for i,patient_labels in enumerate(labels):\n",
    "        test_folds = np.concatenate((test_folds, i*np.ones(patient_labels.shape[0])))\n",
    "    cv_splits = PredefinedSplit(test_folds)\n",
    "    cv_splits = CustomSplitter(cv_splits, labels).split()\n",
    "    return cv_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45860a5d",
   "metadata": {},
   "source": [
    "## Configure CLFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5244eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trans_scorer(score_func):\n",
    "    def scorer(y_true, y_pred):\n",
    "        y_true[y_true == -4]=1\n",
    "        # Set stand_to_walk to always be correct\n",
    "        y_true[(y_true==-3) & (y_pred==1)] = 1\n",
    "        y_true[y_true==-3] = 0\n",
    "        y_true[y_true<0] = 0\n",
    "        return score_func(y_true, y_pred)\n",
    "    return make_scorer(scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe2e288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_svm_clf(cv_splits):\n",
    "    # Configure classifier\n",
    "    parameters = {'C':[0.1,1]}\n",
    "    model = SVC(kernel='rbf',class_weight='balanced')\n",
    "    clf = GridSearchCV(\n",
    "        model, \n",
    "        parameters, \n",
    "        n_jobs = NUM_WORKER,\n",
    "        cv = cv_splits, \n",
    "        scoring=make_trans_scorer(balanced_accuracy_score), \n",
    "        refit=True)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c9796",
   "metadata": {},
   "source": [
    "## Define experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cbcf17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b103419ee3634e64adab0ccf8586939a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = {}\n",
    "base_path = os.path.join('models', 'gait')\n",
    "Path(base_path).mkdir(exist_ok=True, parents=True)\n",
    "for sensors in tqdm(dataset):\n",
    "    path = os.path.join(base_path,f'{sensors}.joblib')\n",
    "    cv_splits = get_cv_splits(labels[sensors])\n",
    "    clf[sensors] = configure_svm_clf(cv_splits)\n",
    "    clf[sensors].fit(data[sensors], target[sensors])\n",
    "    clf[sensors] = SVC(kernel='rbf',class_weight='balanced',**clf[sensors].best_params_)\n",
    "    clf[sensors].fit(data[sensors][target[sensors]>=0], target[sensors][target[sensors]>=0])\n",
    "    dump(clf[sensors], path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160ea8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccec3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
